# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Train a YOLOv5 model on a custom dataset.
Models and datasets download automatically from the latest YOLOv5 release.

Usage - Single-GPU training:
    $ python train.py --data coco128.yaml --weights yolov5s.pt --img 640  # from pretrained (recommended)
    $ python train.py --data coco128.yaml --weights '' --cfg yolov5s.yaml --img 640  # from scratch

Usage - Multi-GPU DDP training:
    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 1 train.py --data coco128.yaml --weights yolov5s.pt --img 640 --device 0,1,2,3

Models:     https://github.com/ultralytics/yolov5/tree/master/models
Datasets:   https://github.com/ultralytics/yolov5/tree/master/data
Tutorial:   https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data
"""

import argparse
import math
import os
import random
import subprocess
import sys
import time
from copy import deepcopy
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.distributed as dist
import torch.nn as nn
import yaml
from torch.optim import lr_scheduler
from tqdm import tqdm

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5 root directory
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # add ROOT to PATH
ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative

import val as validate  # for end-of-epoch mAP
from models.experimental import attempt_load
from models.yolo import Model
from utils.autoanchor import check_anchors
from utils.autobatch import check_train_batch_size
from utils.callbacks import Callbacks
from utils.dataloaders import create_dataloader
from utils.downloads import attempt_download, is_url
from utils.general import (LOGGER, TQDM_BAR_FORMAT, check_amp, check_dataset, check_file, check_git_info,
                           check_git_status, check_img_size, check_requirements, check_suffix, check_yaml, colorstr,
                           get_latest_run, increment_path, init_seeds, intersect_dicts, labels_to_class_weights,
                           labels_to_image_weights, methods, one_cycle, print_args, print_mutation, strip_optimizer,
                           yaml_save)
from utils.loggers import Loggers
from utils.loggers.comet.comet_utils import check_comet_resume
from utils.loss import ComputeLoss
from utils.metrics import fitness
from utils.plots import plot_evolve
from utils.torch_utils import (EarlyStopping, ModelEMA, de_parallel, select_device, smart_DDP, smart_optimizer,
                               smart_resume, torch_distributed_zero_first)
# è·å–æœ¬åœ°è¿›ç¨‹çš„ rankï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™é»˜è®¤ä¸º -1ã€‚
LOCAL_RANK = int(os.getenv('LOCAL_RANK', -1))  # https://pytorch.org/docs/stable/elastic/run.html
# è·å–å…¨å±€è¿›ç¨‹çš„ rankï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™é»˜è®¤ä¸º -1
RANK = int(os.getenv('RANK', -1))
# è·å–å…¨å±€è¿›ç¨‹æ€»æ•°ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®åˆ™é»˜è®¤ä¸º 1
WORLD_SIZE = int(os.getenv('WORLD_SIZE', 1))
# è·å–ä»£ç ä»“åº“çš„ Git ä¿¡æ¯
GIT_INFO = check_git_info()


def train(hyp, opt, device, callbacks):  # hyp is path/to/hyp.yaml or hyp dictionary
    save_dir, epochs, batch_size, weights, single_cls, evolve, data, cfg, resume, noval, nosave, workers, freeze = \
        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.weights, opt.single_cls, opt.evolve, opt.data, opt.cfg, \
        opt.resume, opt.noval, opt.nosave, opt.workers, opt.freeze

    callbacks.run('on_pretrain_routine_start')

    # Directories
    w = save_dir / 'weights'  # weights dir
    (w.parent if evolve else w).mkdir(parents=True, exist_ok=True)  # make dir
    last, best = w / 'last.pt', w / 'best.pt'

    # Hyperparameters
    if isinstance(hyp, str):
        with open(hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # å¦‚æœæ˜¯å­—ç¬¦ä¸²å°†å…¶è§£æä¸ºå­—å…¸ç±»å‹
    LOGGER.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))
    opt.hyp = hyp.copy()  # å°†opt.hypè®¾ç½®ä¸ºè¶…å‚æ•°å­—å…¸çš„å‰¯æœ¬ï¼Œä»¥ä¾¿åœ¨ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ä½¿ç”¨

    # ä¿å­˜æ¨¡å‹çš„è¶…å‚æ•°å’Œè®­ç»ƒé€‰é¡¹åˆ°å¯¹åº”çš„yamlæ–‡ä»¶ä¸­
    if not evolve:
        yaml_save(save_dir / 'hyp.yaml', hyp)
        yaml_save(save_dir / 'opt.yaml', vars(opt))

    # åˆ›å»ºäº†ä¸€ä¸ª Loggers çš„å®ä¾‹å¹¶ä¸”é€šè¿‡ callbacks æ³¨å†Œäº†è¿™ä¸ªå®ä¾‹çš„æ–¹æ³•ä½œä¸ºå›è°ƒå‡½æ•°ï¼Œä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æ—¥å¿—å’Œä¿å­˜æ¨¡å‹ç­‰ä¿¡æ¯
    data_dict = None
    if RANK in {-1, 0}:
        loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance

        # Register actions
        for k in methods(loggers):
            callbacks.register_action(k, callback=getattr(loggers, k))

        # å¦‚æœæ­£åœ¨ä»è¿œç¨‹æ•°æ®é›†ä¸‹è½½æ•°æ®ï¼Œåˆ™å¯ä»¥é€šè¿‡ loggers.remote_dataset å±æ€§æ¥è·å–è¯¥æ•°æ®é›†çš„å­—å…¸
        data_dict = loggers.remote_dataset
        if resume:  # å¦‚æœæ­£åœ¨ä»æ£€æŸ¥ç‚¹æ¢å¤è¿è¡Œï¼Œåˆ™ä¼šå°†å››ä¸ªå‚æ•°çš„å€¼ä»æ£€æŸ¥ç‚¹ä¸­åŠ è½½
            weights, epochs, hyp, batch_size = opt.weights, opt.epochs, opt.hyp, opt.batch_size

    # Config
    plots = not evolve and not opt.noplots  # create plots
    cuda = device.type != 'cpu'
    init_seeds(opt.seed + 1 + RANK, deterministic=True)  # åˆå§‹åŒ–éšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œç»“æœç›¸åŒï¼Œå¯å¤ç°
    # torch_distributed_zero_firstæ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç”¨äºåœ¨PyTorchåˆ†å¸ƒå¼è®­ç»ƒä¸­ç¡®ä¿æŸäº›æ“ä½œåªåœ¨rank=0çš„è¿›ç¨‹ä¸Šæ‰§è¡Œä¸€æ¬¡
    # torch_distributed_zero_firstå°†åœ¨rank=0çš„è¿›ç¨‹ä¸Šæ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨å†…çš„ä»£ç ï¼Œè€Œå…¶ä»–è¿›ç¨‹å°†è·³è¿‡è¯¥ä»£ç å—
    with torch_distributed_zero_first(LOCAL_RANK):
        data_dict = data_dict or check_dataset(data)  # check if None
    train_path, val_path = data_dict['train'], data_dict['val']
    nc = 1 if single_cls else int(data_dict['nc'])  # å¦‚æœæ˜¯å•ç±»åˆ«æ£€æµ‹ï¼Œåˆ™ç±»åˆ«æ•°ä¸º 1ï¼Œå¦åˆ™è·å–æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°
    # å¦‚æœæ˜¯å•ç±»åˆ«æ£€æµ‹ï¼Œä¸”ç±»åˆ«åç§°æ•°é‡ä¸ä¸º 1ï¼Œåˆ™ç±»åˆ«åç§°ä¸ºæ•°æ®é›†ä¸­çš„ç±»åˆ«åç§°ï¼Œå¦åˆ™ä¸ºdata_dict['names']
    names = {0: 'item'} if single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names
    # å¦‚æœéªŒè¯é›†ä¸º COCO æ•°æ®é›†ï¼Œåˆ™ is_coco ä¸º True
    is_coco = isinstance(val_path, str) and val_path.endswith('coco/val2017.txt')  # COCO dataset

    # Model
    # å¦‚æœweightsæ˜¯ä»¥.ptä¸ºåç¼€çš„å­—ç¬¦ä¸²ï¼Œåˆ™ä¼šå°è¯•ä»æœ¬åœ°æˆ–è€…è¿œç¨‹ä¸‹è½½å¯¹åº”çš„é¢„è®­ç»ƒæƒé‡æ–‡ä»¶
    check_suffix(weights, '.pt')  # check weights
    pretrained = weights.endswith('.pt')
    if pretrained:
        with torch_distributed_zero_first(LOCAL_RANK):
            weights = attempt_download(weights)  # æœ¬åœ°æ²¡æ‰¾åˆ°ï¼Œä¸‹è½½
        # å°†æƒé‡åŠ è½½åˆ°CPUä¸Šï¼Œå¹¶æ ¹æ®é…ç½®æ–‡ä»¶æˆ–è€…é¢„è®¾çš„anchorsåˆ›å»ºæ¨¡å‹
        ckpt = torch.load(weights, map_location='cpu')  # ä»æœ¬åœ°æ–‡ä»¶ç³»ç»ŸåŠ è½½é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ weightsï¼Œå°†å…¶åŠ è½½åˆ° CPU å†…å­˜ä¸Šï¼Œä»¥é¿å… CUDA å†…å­˜æ³„æ¼é—®é¢˜
        # æ–°å»ºæ¨¡å‹ï¼Œè™½ç„¶ä¸Šé¢ä¼ å…¥æ¨¡å‹ï¼Œä½†æ˜¯æœ€åncç±»åˆ«æ•°å¹¶ä¸æ˜¯æ¨¡å‹è®­ç»ƒé‚£æ ·ï¼Œä¹Ÿä¸åŒç±»ï¼Œåé¢å°†æ¨¡å‹å‚æ•°è¿ç§»å¹¶è®­ç»ƒï¼Œå› ä¸ºé¢„è®­ç»ƒå¯èƒ½å¯¹æ–°ç±»åˆ«å­¦ä¹ æœ‰å¸®åŠ©
        model = Model(cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create
        # æ ¹æ®è®­ç»ƒé…ç½® hyp å’Œç½‘ç»œé…ç½® cfg ä¸­çš„è®¾å®šï¼Œåˆ›å»ºéœ€è¦å‰”é™¤çš„å‚æ•°é”®ååˆ—è¡¨ exclude
        exclude = ['anchor'] if (cfg or hyp.get('anchors')) and not resume else []  # exclude keys
        csd = ckpt['model'].float().state_dict()  # å°†æ¨¡å‹å‚æ•°çš„ç±»å‹è½¬æ¢ä¸º FP32 æ ¼å¼ï¼Œå­˜å‚¨åœ¨æ–°çš„å­—å…¸å¯¹è±¡ csd ä¸­ï¼Œä»¥åŒ¹é…å½“å‰æ¨¡å‹çš„å‚æ•°ç±»å‹
        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # å–å‡º ckpt ä¸­ä¸å½“å‰æ¨¡å‹å‚æ•°é”®åäº¤é›†çš„å‚æ•°å€¼ï¼Œå­˜å‚¨åœ¨ csd ä¸­
        model.load_state_dict(csd, strict=False)  # å°† csd ä¸­çš„å‚æ•°å€¼åŠ è½½åˆ°å½“å‰æ¨¡å‹ä¸­ï¼Œå‚æ•° strict=False è¡¨ç¤ºå¯ä»¥å…è®¸ä¸å®Œå…¨åŒ¹é…çš„æƒ…å†µï¼Œå³è·³è¿‡éƒ¨åˆ†é”®åä¸åŒ¹é…çš„å‚æ•°
        LOGGER.info(f'Transferred {len(csd)}/{len(model.state_dict())} items from {weights}')  # report
    else:
        # å¦‚æœæ²¡æœ‰é¢„è®­ç»ƒæƒé‡ï¼Œåˆ™ç›´æ¥æ ¹æ®é…ç½®æ–‡ä»¶åˆ›å»ºæ¨¡å‹
        model = Model(cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
    amp = check_amp(model)  # check AMP

    # ç”¨æˆ·åœ¨è®­ç»ƒ YOLOv5 æ¨¡å‹æ—¶é€‰æ‹©éœ€è¦å†»ç»“çš„å±‚ä»¥æé«˜è®­ç»ƒæ•ˆæœ
    # å¦‚æœ freeze åˆ—è¡¨é•¿åº¦å¤§äº 1ï¼Œåˆ™ä½¿ç”¨åˆ—è¡¨ä¸­çš„å€¼ä½œä¸ºå±‚æ•°ï¼›å¦åˆ™ï¼Œä½¿ç”¨ freeze[0] ä½œä¸ºå±‚æ•°
    freeze = [f'model.{x}.' for x in (freeze if len(freeze) > 1 else range(freeze[0]))]  # layers to freeze
    for k, v in model.named_parameters():
        v.requires_grad = True  # é€šè¿‡éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å‚æ•°ï¼Œå¹¶å°† requires_grad è®¾ç½®ä¸º Trueï¼Œæ¥è®­ç»ƒæ‰€æœ‰å±‚
        # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
        # å¦‚æœå‚æ•°ååŒ…å«åœ¨ freeze åˆ—è¡¨ä¸­ï¼Œåˆ™å°† requires_grad è®¾ç½®ä¸º Falseï¼Œä»è€Œå†»ç»“è¯¥å±‚ã€‚
        if any(x in k for x in freeze):
            LOGGER.info(f'freezing {k}')
            v.requires_grad = False

    # Image size
    gs = max(int(model.stride.max()), 32)  # è®¾ç½®æ¨¡å‹çš„æœ€å¤§æ­¥é•¿
    imgsz = check_img_size(opt.imgsz, gs, floor=gs * 2)  # æ£€æŸ¥å›¾åƒå¤§å°ï¼Œä»¥ç¡®ä¿å®ƒæ˜¯ç½‘æ ¼å¤§å°çš„å€æ•°

    # åŸºäºæ¨¡å‹ã€å›¾åƒå¤§å°å’Œè‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆampï¼‰è®¾ç½®æ¥ä¼°è®¡è®­ç»ƒçš„æœ€ä½³æ‰¹é‡å¤§å°
    if RANK == -1 and batch_size == -1:  # å¦‚æœåœ¨å•ä¸ªGPUä¸Šè¿è¡Œï¼Œå®ƒå°†è‡ªåŠ¨ä¼°è®¡æœ€ä½³æ‰¹å¤„ç†å¤§å°
        batch_size = check_train_batch_size(model, imgsz, amp)
        loggers.on_params_update({"batch_size": batch_size})

    # ä¼˜åŒ–å™¨
    nbs = 64  # ä¼˜åŒ–å™¨çš„æ‰¹é‡å¤§å°ä¸º64
    accumulate = max(round(nbs / batch_size), 1)  # è¡¨ç¤ºåœ¨ä¼˜åŒ–ä¹‹å‰ç´¯ç§¯æŸå¤±çš„æ‰¹é‡å¤§å°
    hyp['weight_decay'] *= batch_size * accumulate / nbs  # æƒé‡è¡°å‡çš„ç¼©æ”¾å› å­
    # åˆ›å»ºä¼˜åŒ–å™¨å¯¹è±¡
    optimizer = smart_optimizer(model, opt.optimizer, hyp['lr0'], hyp['momentum'], hyp['weight_decay'])

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    if opt.cos_lr:
        lf = one_cycle(1, hyp['lrf'], epochs)  # cosine 1->hyp['lrf']ï¼Œä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨
    else:
        lf = lambda x: (1 - x / epochs) * (1.0 - hyp['lrf']) + hyp['lrf']  # linear çº¿æ€§è°ƒåº¦å™¨
    # ä½¿ç”¨ `LambdaLR` ç±»åˆ›å»ºä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨å¯¹è±¡ï¼Œå¹¶ä¼ å…¥ä¼˜åŒ–å™¨å’Œ `lf` å‡½æ•°
    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)

    # EMA
    # å¦‚æœæ˜¯ä¸»è¿›ç¨‹ï¼Œå³ RANK ä¸º -1 æˆ– 0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª ModelEMA ç±»çš„å®ä¾‹å¯¹è±¡ ema
    # EMA)æŠ€æœ¯æ˜¯ä¸€ç§å¸¸è§çš„ä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºå¹³æ»‘è®¡ç®—ä¸­çš„å˜åŒ–ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¸¸ç”¨äºæ¨¡å‹æƒé‡æ›´æ–°çš„å¹³æ»‘ï¼Œä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŠ–åŠ¨
    # åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸é‡‡ç”¨éšæœºæ¢¯åº¦ä¸‹é™ (SGD) æˆ–å…¶å˜ä½“ä½œä¸ºæ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–ç®—æ³•ã€‚åœ¨ä½¿ç”¨è¿™äº›ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒæ—¶ï¼Œæ¨¡å‹å‚æ•°ä¼šåœ¨æ¯ä¸ªæ‰¹æ¬¡æˆ–æ¯ä¸ª epoch ä¸­è¢«æ›´æ–°ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„é¢‘ç¹æ›´æ–°å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å‚æ•°æ³¢åŠ¨æˆ–éœ‡è¡ï¼Œä»è€Œå½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚
    # EMA æŠ€æœ¯é€šè¿‡å¯¹æ¨¡å‹å‚æ•°çš„ç§»åŠ¨å¹³å‡æ¥å‡è½»è¿™ç§æ³¢åŠ¨å’Œéœ‡è¡ï¼Œä½¿å¾—æ¨¡å‹çš„æ›´æ–°æ›´åŠ å¹³æ»‘
    # EMA ç»´æŠ¤äº†ä¸€ä¸ªæŒ‡æ•°åŠ æƒå¹³å‡ (exponentially weighted average) çš„ç¼“å­˜ï¼Œç”¨äºè®°å½•æ¯ä¸ªå‚æ•°çš„å†å²å˜åŒ–è¶‹åŠ¿
    # åœ¨æ›´æ–°æ¨¡å‹å‚æ•°æ—¶ï¼ŒEMA ä¼šåŒæ—¶æ›´æ–°æŒ‡æ•°åŠ æƒå¹³å‡çš„å€¼ï¼Œä»¥æ›´åŠ å¹³æ»‘åœ°æ›´æ–°æ¨¡å‹å‚æ•°
    # é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒEMA æŠ€æœ¯å¯ä»¥å¸®åŠ©ä¼˜åŒ–ç®—æ³•æ›´å¥½åœ°æ”¶æ•›ï¼Œä»è€Œæé«˜æ¨¡å‹çš„æ€§èƒ½
    ema = ModelEMA(model) if RANK in {-1, 0} else None

    # Resume
    # åˆå§‹åŒ–æœ€ä½³ç²¾åº¦å’Œå¼€å§‹çš„è®­ç»ƒ epoch
    best_fitness, start_epoch = 0.0, 0
    if pretrained:
        if resume:
            # è°ƒç”¨ smart_resume å‡½æ•°æ¥æ‰§è¡Œä»æ–­ç‚¹å¤„ç»§ç»­è®­ç»ƒçš„é€»è¾‘ï¼Œå¹¶å°†è¿”å›çš„ best_fitnessã€start_epoch å’Œ epochs åˆ†åˆ«èµ‹å€¼ç»™å˜é‡
            best_fitness, start_epoch, epochs = smart_resume(ckpt, optimizer, ema, weights, epochs, resume)
        del ckpt, csd

    # å¤šå¡æ‰éœ€è¦DPå’ŒSyncBatchNorm
    # DP mode
    if cuda and RANK == -1 and torch.cuda.device_count() > 1:
        LOGGER.warning('WARNING âš ï¸ DP not recommended, use torch.distributed.run for best DDP Multi-GPU results.\n'
                       'See Multi-GPU Tutorial at https://github.com/ultralytics/yolov5/issues/475 to get started.')
        # å½“è¿­ä»£æ¬¡æ•°æˆ–è€…epochè¶³å¤Ÿå¤§çš„æ—¶å€™ï¼Œç”¨nn.DataParallelå‡½æ•°æ¥ç”¨å¤šä¸ªGPUæ¥åŠ é€Ÿè®­ç»ƒ
        # DataParallel ä¼šè‡ªåŠ¨å¸®æˆ‘ä»¬å°†æ•°æ®åˆ‡åˆ† load åˆ°ç›¸åº” GPUï¼Œå°†æ¨¡å‹å¤åˆ¶åˆ°ç›¸åº” GPUï¼Œè¿›è¡Œæ­£å‘ä¼ æ’­è®¡ç®—æ¢¯åº¦å¹¶æ±‡æ€»
        model = torch.nn.DataParallel(model)

    # SyncBatchNorm
    if opt.sync_bn and cuda and RANK != -1:
        # å°†æ¨¡å‹ä¸­çš„torch.nn.BatchNormNDå±‚è½¬æ¢ä¸ºtorch.nn.SyncBatchNormå±‚ã€‚
        # å¼•å…¥SyncBNï¼Œè·Ÿä¸€èˆ¬æ‰€è¯´çš„æ™®é€šBNçš„ä¸åŒåœ¨äºå·¥ç¨‹å®ç°æ–¹å¼ï¼šSyncBNèƒ½å¤Ÿå®Œç¾æ”¯æŒå¤šå¡è®­ç»ƒï¼Œè€Œæ™®é€šBNåœ¨å¤šå¡æ¨¡å¼ä¸‹å®é™…ä¸Šå°±æ˜¯å•å¡æ¨¡å¼
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)
        LOGGER.info('Using SyncBatchNorm()')

    # Trainloader
    # dataloaderç†è§£ä¸ºæ•°æ®æŠ“å–å™¨
    train_loader, dataset = create_dataloader(train_path,
                                              imgsz,
                                              batch_size // WORLD_SIZE,
                                              gs,
                                              single_cls,
                                              hyp=hyp,
                                              augment=True,
                                              cache=None if opt.cache == 'val' else opt.cache,
                                              rect=opt.rect,
                                              rank=LOCAL_RANK,
                                              workers=workers,
                                              image_weights=opt.image_weights,
                                              quad=opt.quad,
                                              prefix=colorstr('train: '),
                                              shuffle=True,
                                              seed=opt.seed)
    # å°†æ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾æ‹¼æ¥åˆ°ä¸€èµ·shapeä¸º(total, 5)ï¼Œç»Ÿè®¡ååšå¯è§†åŒ–
    # è·å–æ ‡ç­¾ä¸­æœ€å¤§çš„ç±»åˆ«å€¼ï¼Œå¹¶äºç±»åˆ«æ•°ä½œæ¯”è¾ƒ
    # å¦‚æœå¤§äºç±»åˆ«æ•°åˆ™è¡¨ç¤ºæœ‰é—®é¢˜
    labels = np.concatenate(dataset.labels, 0)
    mlc = int(labels[:, 0].max())  # max label class
    assert mlc < nc, f'Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}'

    # Process 0
    if RANK in {-1, 0}:
        val_loader = create_dataloader(val_path,
                                       imgsz,
                                       batch_size // WORLD_SIZE * 2,
                                       gs,
                                       single_cls,
                                       hyp=hyp,
                                       cache=None if noval else opt.cache,
                                       rect=True,
                                       rank=-1,
                                       workers=workers * 2,
                                       pad=0.5,
                                       prefix=colorstr('val: '))[0]

        if not resume:
            # å¦‚æœä¸æ˜¯æ¢å¤è®­ç»ƒï¼Œåˆ™æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨è®¾ç½®anchorï¼Œå¹¶è°ƒç”¨ check_anchors() å‡½æ•°è¿›è¡Œè®¾ç½®ã€‚
            if not opt.noautoanchor:
                check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)  # run AutoAnchor
            model.half().float()  # pre-reduce anchor precision å°†æ¨¡å‹çš„anchorå‚æ•°çš„ç²¾åº¦é™ä½è‡³FP16ï¼Œä»¥å‡å°‘å†…å­˜æ¶ˆè€—
        # ç”¨äºæ‰§è¡Œè®­ç»ƒå‰çš„å‡†å¤‡å·¥ä½œï¼Œä¾‹å¦‚å¯¹æ ‡ç­¾å’Œç±»åˆ«åç§°çš„å¤„ç†
        callbacks.run('on_pretrain_routine_end', labels, names)

    # DDP mode
    if cuda and RANK != -1:
        model = smart_DDP(model)

    # å°†ä¸€äº›æ¨¡å‹å±æ€§å’Œè¶…å‚æ•°æ·»åŠ åˆ°æ¨¡å‹
    nl = de_parallel(model).model[-1].nl  # æ£€æµ‹å±‚çš„æ•°é‡ï¼ˆnlï¼‰ï¼Œç”¨äºåœ¨è¶…å‚æ•°ï¼ˆhypï¼‰ï¼Œä½¿å®ƒä»¬é€‚åº”ä¸åŒçš„æ£€æµ‹å±‚
    # ä¸‹é¢ä¸‰ä¸ªè¶…å‚æ•°å«ä¹‰ä¸ºæŸå¤±å‡½æ•°å› å­
    hyp['box'] *= 3 / nl  # ç¼©æ”¾ç›®æ ‡æ¡†ï¼ˆboxï¼‰
    hyp['cls'] *= nc / 80 * 3 / nl  # ç±»åˆ«é¢„æµ‹ï¼ˆclsï¼‰
    hyp['obj'] *= (imgsz / 640) ** 2 * 3 / nl  # scale to image size and layers,ç‰©ä½“ç½®ä¿¡åº¦ï¼ˆobjï¼‰çš„æƒé‡
    hyp['label_smoothing'] = opt.label_smoothing  # æ ‡ç­¾å¹³æ»‘å‚æ•°ï¼ˆlabel_smoothingï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå‡è½»è¿‡æ‹Ÿåˆçš„æŠ€å·§ï¼Œå®ƒå°†çœŸå®æ ‡ç­¾å€¼å‘å…¶ä»–ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒå¹³æ»‘åŒ–
    model.nc = nc  # æ¨¡å‹çš„ç±»åˆ«æ•°é‡ï¼ˆncï¼‰ï¼Œå°†å…¶é™„åŠ åˆ°æ¨¡å‹ä¸­
    model.hyp = hyp  # attach hyperparameters to model
    # æ¯ä¸ªç±»åˆ«çš„æƒé‡ï¼ˆclass_weightsï¼‰ï¼Œç”¨äºè®¡ç®—æŸå¤±å‡½æ•°æ—¶å¹³è¡¡ç±»åˆ«ä¹‹é—´çš„è´¡çŒ®
    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device) * nc  # attach class weights
    model.names = names

    # Start training
    t0 = time.time()
    nb = len(train_loader)  # number of batches
    nw = max(round(hyp['warmup_epochs'] * nb), 100)  # number of warmup iterations, max(3 epochs, 100 iterations)
    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training
    last_opt_step = -1  # ä¸Šæ¬¡æ›´æ–°å‚æ•°è®¡æ•°å™¨å€¼
    maps = np.zeros(nc)  # mAP per classï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­mapå€¼
    # ç»“æœ
    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)
    scheduler.last_epoch = start_epoch - 1  # do not move
    scaler = torch.cuda.amp.GradScaler(enabled=amp)  # è‡ªåŠ¨å›å½’ç²¾åº¦è®­ç»ƒ
    stopper, stop = EarlyStopping(patience=opt.patience), False  # æå‰ç»ˆæ­¢
    compute_loss = ComputeLoss(model)  # åˆå§‹æŸå¤±å‡½æ•°
    callbacks.run('on_train_start')
    LOGGER.info(f'Image sizes {imgsz} train, {imgsz} val\n'
                f'Using {train_loader.num_workers * WORLD_SIZE} dataloader workers\n'
                f"Logging results to {colorstr('bold', save_dir)}\n"
                f'Starting training for {epochs} epochs...')
    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------
        callbacks.run('on_train_epoch_start')
        model.train()

        # è®­ç»ƒæ—¶ç”±äºå›¾ç‰‡è¯†åˆ«ç›®æ ‡å¯èƒ½è¯†åˆ«æ¯”è¾ƒå®¹æ˜“ï¼Œæœ‰çš„å¯èƒ½æ¯”è¾ƒå›°éš¾ï¼Œå› æ­¤å¯ä»¥ç»™æ¨¡å‹åˆ†é…é‡‡æ ·æƒé‡ï¼Œä½¿æ¨¡å‹é¢å¤–å…³æ³¨éš¾è¯†åˆ«æ ·æœ¬
        # Update image weights (optional, single-GPU only)
        if opt.image_weights:
            # æŸä¸€ç±»æ•°é‡æƒé‡
            cw = model.class_weights.cpu().numpy() * (1 - maps) ** 2 / nc  # ç±»åˆ«æƒé‡ï¼Œå¢å¤§ç±»åˆ«æƒé‡å¯ä»¥å¢å¤§éš¾é‡‡æ ·éƒ¨åˆ†è¢«è¯†åˆ«æ¦‚ç‡
            iw = labels_to_image_weights(dataset.labels, nc=nc, class_weights=cw)  # ç±»åˆ«æƒé‡æ¢ç®—åˆ°å›¾ç‰‡ç»´åº¦ï¼Œå³æ¯å¼ å›¾ç‰‡é‡‡æ ·æƒé‡
            dataset.indices = random.choices(range(dataset.n), weights=iw, k=dataset.n)  # å¢åŠ æƒé‡åå†éšæœºé‡é‡‡æ ·ï¼Œåé¢ä¸€æ‰¹æ‰¹åˆ†æå›¾ç‰‡ä¸­ï¼Œéš¾è¯†åˆ«æ ·æœ¬æ•°ä¼šå¢åŠ 

        # Update mosaic border (optional)
        # b = int(random.uniform(0.25 * imgsz, 0.75 * imgsz + gs) // gs * gs)
        # dataset.mosaic_border = [b - imgsz, -b]  # height, width borders

        mloss = torch.zeros(3, device=device)  # åˆå§‹æŸå¤±å€¼ï¼Œæ¡†å›å½’æŸå¤±ï¼Œç±»åˆ«æŸå¤±å’Œç½®ä¿¡åº¦æŸå¤±ï¼Œ3ç§
        if RANK != -1:
            train_loader.sampler.set_epoch(epoch)
        pbar = enumerate(train_loader)
        # å±•ç¤ºè®­ç»ƒè¿›åº¦
        LOGGER.info(('\n' + '%11s' * 7) % ('Epoch', 'GPU_mem', 'box_loss', 'obj_loss', 'cls_loss', 'Instances', 'Size'))
        if RANK in {-1, 0}:
            pbar = tqdm(pbar, total=nb, bar_format=TQDM_BAR_FORMAT)  # progress bar
        optimizer.zero_grad()  # æ¢¯åº¦å½’é›¶
        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------
            callbacks.run('on_train_batch_start')
            ni = i + nb * epoch  # number integrated batches (since train start)
            imgs = imgs.to(device, non_blocking=True).float() / 255  # uint8 to float32, 0-255 to 0.0-1.0

            # çƒ­èº«ï¼Œå‰é¢å‡ æ¬¡å­¦ä¹ ç‡ä¸é‚£ä¹ˆå¤§
            if ni <= nw:
                xi = [0, nw]  # x interp
                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)
                accumulate = max(1, np.interp(ni, xi, [1, nbs / batch_size]).round())
                for j, x in enumerate(optimizer.param_groups):
                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 0 else 0.0, x['initial_lr'] * lf(epoch)])
                    if 'momentum' in x:
                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])

            # å¤šå°ºåº¦è®­ç»ƒ
            if opt.multi_scale:
                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size
                sf = sz / max(imgs.shape[2:])  # scale factor
                if sf != 1:
                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)
                    imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)

            # Forward
            with torch.cuda.amp.autocast(amp):
                pred = model(imgs)  # forward
                loss, loss_items = compute_loss(pred, targets.to(device))  # é¢„æµ‹æ¡†å’Œæ ‡æ³¨æ¡†ç›´æ¥æŸå¤±å€¼
                if RANK != -1:
                    loss *= WORLD_SIZE  # gradient averaged between devices in DDP mode
                if opt.quad:
                    loss *= 4.

            # Backward
            scaler.scale(loss).backward()

            # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
            # å¤šæ‰¹æ•°æ®è¿›è¡Œç´¯ç§¯ï¼Œç»Ÿä¸€è¿›è¡Œæ›´æ–°
            if ni - last_opt_step >= accumulate:
                scaler.unscale_(optimizer)  # å°†ä¼˜åŒ–å™¨ä¸­çš„æ¢¯åº¦è¿›è¡Œåå½’ä¸€åŒ–å¤„ç†
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦è¿›è¡Œè£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                scaler.step(optimizer)  # ä½¿ç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œæ›´æ–°
                scaler.update()  # æ›´æ–°scalerï¼Œä»¥ä¾¿ä¸‹ä¸€æ¬¡ä½¿ç”¨
                optimizer.zero_grad()  # æ¸…ç©ºä¼˜åŒ–å™¨ä¸­çš„æ¢¯åº¦ä¿¡æ¯
                if ema:
                    ema.update(model)  # æ¨¡å‹è¿›è¡ŒæŒ‡æ•°ç§»åŠ¨å¹³å‡å¤„ç†
                last_opt_step = ni

            # Log
            if RANK in {-1, 0}:
                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses
                mem = f'{torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0:.3g}G'  # (GB)
                pbar.set_description(('%11s' * 2 + '%11.4g' * 5) %
                                     (f'{epoch}/{epochs - 1}', mem, *mloss, targets.shape[0], imgs.shape[-1]))
                # åœ¨loggers/_init_.pyä¸­æœ‰åŒåå‡½æ•°ï¼Œè°ƒç”¨
                callbacks.run('on_train_batch_end', model, ni, imgs, targets, paths, list(mloss))
                if callbacks.stop_training:
                    return
            # end batch ------------------------------------------------------------------------------------------------

        # Scheduler
        lr = [x['lr'] for x in optimizer.param_groups]  # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()

        if RANK in {-1, 0}:
            # mAP
            callbacks.run('on_train_epoch_end', epoch=epoch)
            # emaæ·»åŠ å±æ€§
            ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])
            # åˆ¤æ–­æ˜¯å¦æœ€ç»ˆä¸€è½®
            final_epoch = (epoch + 1 == epochs) or stopper.possible_stop
            if not noval or final_epoch:  # Calculate mAP
                # ä¸æ˜¯æœ€åä¸€è½®ï¼Œåœ¨éªŒè¯é›†å†è·‘ä¸€é
                results, maps, _ = validate.run(data_dict,
                                                batch_size=batch_size // WORLD_SIZE * 2,
                                                imgsz=imgsz,
                                                half=amp,
                                                model=ema.ema,
                                                single_cls=single_cls,
                                                dataloader=val_loader,
                                                save_dir=save_dir,
                                                plots=False,
                                                callbacks=callbacks,
                                                compute_loss=compute_loss)

            # Update best mAP
            # è®¡ç®—æ‹Ÿåˆåº¦ï¼Œå¯¹å¤šä¸ªæŒ‡æ ‡åŠ æƒæ±‚å’Œ
            fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]
            stop = stopper(epoch=epoch, fitness=fi)  # early stop check
            # åˆ¤æ–­æœ€å¥½ï¼Œæ˜¯å¦è®°å½•ä¸‹æ¥
            if fi > best_fitness:
                best_fitness = fi
            log_vals = list(mloss) + list(results) + lr
            callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)

            # Save model
            if (not nosave) or (final_epoch and not evolve):  # if save
                ckpt = {
                    'epoch': epoch,
                    'best_fitness': best_fitness,
                    'model': deepcopy(de_parallel(model)).half(),
                    'ema': deepcopy(ema.ema).half(),
                    'updates': ema.updates,
                    'optimizer': optimizer.state_dict(),
                    'opt': vars(opt),
                    'git': GIT_INFO,  # {remote, branch, commit} if a git repo
                    'date': datetime.now().isoformat()}

                # Save last, best and delete
                torch.save(ckpt, last)
                if best_fitness == fi:
                    torch.save(ckpt, best)
                if opt.save_period > 0 and epoch % opt.save_period == 0:
                    torch.save(ckpt, w / f'epoch{epoch}.pt')
                del ckpt
                callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)

        # åˆ¤æ–­æ‹Ÿåˆåº¦æ˜¯å¦éƒ½æ²¡ä¸Šå‡ï¼Œæ˜¯å¦æå‰è®­ç»ƒç»“æŸï¼Œ
        if RANK != -1:  # if DDP training
            broadcast_list = [stop if RANK == 0 else None]
            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
            if RANK != 0:
                stop = broadcast_list[0]
        if stop:
            break  # must break all DDP ranks

        # end epoch ----------------------------------------------------------------------------------------------------
    # end training -----------------------------------------------------------------------------------------------------
    if RANK in {-1, 0}:
        LOGGER.info(f'\n{epoch - start_epoch + 1} epochs completed in {(time.time() - t0) / 3600:.3f} hours.')
        for f in last, best:
            if f.exists():
                strip_optimizer(f)  # strip optimizers
                if f is best:
                    LOGGER.info(f'\nValidating {f}...')
                    results, _, _ = validate.run(
                        data_dict,
                        batch_size=batch_size // WORLD_SIZE * 2,
                        imgsz=imgsz,
                        model=attempt_load(f, device).half(),
                        iou_thres=0.65 if is_coco else 0.60,  # best pycocotools at iou 0.65
                        single_cls=single_cls,
                        dataloader=val_loader,
                        save_dir=save_dir,
                        save_json=is_coco,
                        verbose=True,
                        plots=plots,
                        callbacks=callbacks,
                        compute_loss=compute_loss)  # val best model with plots
                    if is_coco:
                        callbacks.run('on_fit_epoch_end', list(mloss) + list(results) + lr, epoch, best_fitness, fi)

        callbacks.run('on_train_end', last, best, epoch, results)

    torch.cuda.empty_cache()
    return results


def parse_opt(known=False):
    parser = argparse.ArgumentParser()
    # æƒé‡æ–‡ä»¶è·¯å¾„
    parser.add_argument('--weights', type=str, default=ROOT / 'weights/yolov5s.pt', help='initial weights path')
    # å­˜å‚¨æ¨¡å‹ç»“æ„çš„é…ç½®æ–‡ä»¶ï¼ŒæŒ‡å®šäº†ä¸€äº›å‚æ•°ä¿¡æ¯å’Œbackboneçš„ç»“æ„ä¿¡æ¯
    parser.add_argument('--cfg', type=str, default=ROOT / 'models/yolov5s-fire.yaml', help='model.yaml path')
    # å­˜å‚¨è®­ç»ƒã€æµ‹è¯•æ•°æ®çš„æ–‡ä»¶
    parser.add_argument('--data', type=str, default=ROOT / 'data/fire-smoke.yaml', help='dataset.yaml path')
    # æ¨¡å‹çš„è¶…å‚æ•°è·¯å¾„
    parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
    # epochã€batchsizeã€iterationä¸‰è€…ä¹‹é—´çš„è”ç³»
    # 1ã€batchsizeæ˜¯æ‰¹æ¬¡å¤§å°ï¼Œå‡å¦‚å–batchsize=24ï¼Œåˆ™è¡¨ç¤ºæ¯æ¬¡è®­ç»ƒæ—¶åœ¨è®­ç»ƒé›†ä¸­å–24ä¸ªè®­ç»ƒæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚
    # 2ã€iterationæ˜¯è¿­ä»£æ¬¡æ•°ï¼Œ1ä¸ªiterationå°±ç­‰äºä¸€æ¬¡ä½¿ç”¨24ï¼ˆbatchsizeå¤§å°ï¼‰ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚
    # 3ã€epochï¼š1ä¸ªepochå°±ç­‰äºä½¿ç”¨è®­ç»ƒé›†ä¸­å…¨éƒ¨æ ·æœ¬è®­ç»ƒ1æ¬¡
    # è®­ç»ƒè¿‡ç¨‹ä¸­æ•´ä¸ªæ•°æ®é›†å°†è¢«è¿­ä»£å¤šå°‘æ¬¡
    parser.add_argument('--epochs', type=int, default=100, help='total training epochs')
    # ä¸€æ¬¡çœ‹å®Œå¤šå°‘å¼ å›¾ç‰‡æ‰è¿›è¡Œæƒé‡æ›´æ–°ï¼Œæ¢¯åº¦ä¸‹é™çš„mini-batch
    parser.add_argument('--batch-size', type=int, default=8, help='total batch size for all GPUs, -1 for autobatch')
    # è¾“å…¥å›¾ç‰‡å®½é«˜
    parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    # parser.add_argument('--weights', type=str, default=ROOT / 'yolov5s.pt', help='initial weights path')
    # parser.add_argument('--cfg', type=str, default='', help='model.yaml path')
    # parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='dataset.yaml path')
    # parser.add_argument('--hyp', type=str, default=ROOT / 'data/hyps/hyp.scratch-low.yaml', help='hyperparameters path')
    # parser.add_argument('--epochs', type=int, default=100, help='total training epochs')
    # parser.add_argument('--batch-size', type=int, default=16, help='total batch size for all GPUs, -1 for autobatch')
    # parser.add_argument('--imgsz', '--img', '--img-size', type=int, default=640, help='train, val image size (pixels)')
    # è¿›è¡ŒçŸ©å½¢è®­ç»ƒ
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    # æ–­ç‚¹ç»­è®­ï¼šå³æ˜¯å¦åœ¨ä¹‹å‰è®­ç»ƒçš„ä¸€ä¸ªæ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼Œdefault å€¼é»˜è®¤æ˜¯ Falseï¼›
    # å¦‚æœæƒ³é‡‡ç”¨æ–­ç‚¹ç»­è®­çš„æ–¹å¼ï¼Œæ¨èå°† default=False æ”¹ä¸º default=Trueã€‚
    # éšååœ¨ç»ˆç«¯ä¸­é”®å…¥å¦‚ä¸‹æŒ‡ä»¤ï¼š
    # python train.py --resume D:\Pycharm_Projects\yolov5-6.1-4_23\runs\train\exp19\weights\last.ptä¸Šä¸€æ¬¡ä¸­æ–­æ—¶ä¿å­˜çš„ptæ–‡ä»¶è·¯å¾„
    parser.add_argument('--resume', nargs='?', const=True, default=False, help='resume most recent training')
    # ä»…ä¿å­˜æœ€ç»ˆcheckpoint
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    # ä»…éªŒè¯æœ€åä¸€ä¸ª epochï¼Œè€Œä¸æ˜¯æ¯ä¸ª epoch éƒ½è¿›è¡ŒéªŒè¯
    parser.add_argument('--noval', action='store_true', help='only validate final epoch')
    # ç¦ç”¨è‡ªåŠ¨é”šæ¡†ï¼ˆAutoAnchorï¼‰çš„åŠŸèƒ½ï¼Œè‡ªåŠ¨é”šç‚¹çš„å¥½å¤„æ˜¯å¯ä»¥ç®€åŒ–è®­ç»ƒè¿‡ç¨‹
    # è‡ªåŠ¨é”šå®šæ¡†é€‰é¡¹ï¼Œè®­ç»ƒå¼€å§‹å‰ï¼Œä¼šè‡ªåŠ¨è®¡ç®—æ•°æ®é›†æ ‡æ³¨ä¿¡æ¯é’ˆå¯¹é»˜è®¤é”šå®šæ¡†çš„æœ€ä½³å¬å›ç‡ï¼Œå½“æœ€ä½³å¬å›ç‡å¤§äºç­‰äº0.98æ—¶ï¼Œåˆ™ä¸éœ€è¦æ›´æ–°é”šå®šæ¡†ï¼›
    # å¦‚æœæœ€ä½³å¬å›ç‡å°äº0.98ï¼Œåˆ™éœ€è¦é‡æ–°è®¡ç®—ç¬¦åˆæ­¤æ•°æ®é›†çš„é”šå®šæ¡†
    parser.add_argument('--noautoanchor', action='store_true', help='disable AutoAnchor')
    # ç¦æ­¢ä¿å­˜ç»˜å›¾æ–‡ä»¶
    parser.add_argument('--noplots', action='store_true', help='save no plot files')
    # å¤šå°‘ä»£è¿›åŒ–ä¸€æ¬¡è¶…å‚æ•°
    # yolov5ä½¿ç”¨é—ä¼ è¶…å‚æ•°è¿›åŒ–ï¼Œæä¾›çš„é»˜è®¤å‚æ•°æ˜¯é€šè¿‡åœ¨COCOæ•°æ®é›†ä¸Šä½¿ç”¨è¶…å‚æ•°è¿›åŒ–å¾—æ¥çš„ã€‚ç”±äºè¶…å‚æ•°è¿›åŒ–ä¼šè€—è´¹å¤§é‡çš„èµ„æºå’Œæ—¶é—´ï¼Œæ‰€ä»¥å»ºè®®å¤§å®¶ä¸è¦åŠ¨è¿™ä¸ªå‚æ•°
    parser.add_argument('--evolve', type=int, nargs='?', const=300, help='evolve hyperparameters for x generations')
    # æŒ‡å®šä¸€ä¸ª Google Cloud Storage å­˜å‚¨æ¡¶
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    # æŒ‡å®šå›¾åƒçš„ç¼“å­˜ç±»å‹ï¼Œå‚æ•°æœªæŒ‡å®šæ—¶çš„é»˜è®¤å€¼ä¸º 'ram'
    # æ˜¯å¦æå‰ç¼“å­˜å›¾ç‰‡åˆ°å†…å­˜ï¼Œä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œé»˜è®¤Falseï¼›å¼€å¯è¿™ä¸ªå‚æ•°å°±ä¼šå¯¹å›¾ç‰‡è¿›è¡Œç¼“å­˜ï¼Œä»è€Œæ›´å¥½çš„è®­ç»ƒæ¨¡å‹
    parser.add_argument('--cache', type=str, nargs='?', const='ram', help='image --cache ram/disk')
    # æŒ‡å®šåœ¨è®­ç»ƒæ—¶ä½¿ç”¨åŠ æƒå›¾åƒé€‰æ‹©
    # æ˜¯å¦å¯ç”¨åŠ æƒå›¾åƒç­–ç•¥ï¼Œé»˜è®¤æ˜¯ä¸å¼€å¯çš„ï¼›ä¸»è¦æ˜¯ä¸ºäº†è§£å†³æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜ï¼›å¼€å¯åä¼šå¯¹äºä¸Šä¸€è½®è®­ç»ƒæ•ˆæœä¸å¥½çš„å›¾ç‰‡ï¼Œåœ¨ä¸‹ä¸€è½®ä¸­å¢åŠ ä¸€äº›æƒé‡
    parser.add_argument('--image-weights', action='store_true', help='use weighted image selection for training')
    # cudaè®¾å¤‡, i.e. 0 or 0,1,2,3 or cpu
    parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    # å¤šå°ºåº¦è®­ç»ƒï¼Œimg-size +/- 50%
    # æ˜¯å¦å¯ç”¨å¤šå°ºåº¦è®­ç»ƒï¼Œé»˜è®¤æ˜¯ä¸å¼€å¯çš„ï¼›å¤šå°ºåº¦è®­ç»ƒæ˜¯æŒ‡è®¾ç½®å‡ ç§ä¸åŒçš„å›¾ç‰‡è¾“å…¥å°ºåº¦ï¼Œè®­ç»ƒæ—¶æ¯éš”ä¸€å®šiterationséšæœºé€‰å–ä¸€ç§å°ºåº¦è®­ç»ƒï¼Œè¿™æ ·è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹é²æ£’æ€§æ›´å¼º
    # å¤šå°ºåº¦è®­ç»ƒåœ¨æ¯”èµ›ä¸­ç»å¸¸å¯ä»¥çœ‹åˆ°ä»–èº«å½±ï¼Œæ˜¯è¢«è¯æ˜äº†æœ‰æ•ˆæé«˜æ€§èƒ½çš„æ–¹å¼ã€‚è¾“å…¥å›¾ç‰‡çš„å°ºå¯¸å¯¹æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½å½±å“å¾ˆå¤§ï¼Œåœ¨åŸºç¡€ç½‘ç»œéƒ¨åˆ†å¸¸å¸¸ä¼šç”Ÿæˆæ¯”åŸå›¾å°æ•°åå€çš„ç‰¹å¾å›¾ï¼Œå¯¼è‡´å°ç‰©ä½“çš„ç‰¹å¾æè¿°ä¸å®¹æ˜“è¢«æ£€æµ‹ç½‘ç»œæ•æ‰ã€‚é€šè¿‡è¾“å…¥æ›´å¤§ã€æ›´å¤šå°ºå¯¸çš„å›¾ç‰‡è¿›è¡Œè®­ç»ƒï¼Œèƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šæé«˜æ£€æµ‹æ¨¡å‹å¯¹ç‰©ä½“å¤§å°çš„é²æ£’æ€§ã€‚
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    # å•ç±»åˆ«çš„è®­ç»ƒé›†ï¼Œå•ç±»åˆ«è¿˜æ˜¯å¤šç±»åˆ«ï¼›é»˜è®¤ä¸ºFalseå¤šç±»åˆ«
    parser.add_argument('--single-cls', action='store_true', help='train multi-class data as single-class')
    # æŒ‡å®šä¼˜åŒ–å™¨çš„ç±»å‹ï¼Œé€‰æ‹©ä¼˜åŒ–å™¨ï¼›é»˜è®¤ä¸ºSGDï¼Œå¯é€‰SGDï¼ŒAdamï¼ŒAdamW ã€‚
    parser.add_argument('--optimizer', type=str, choices=['SGD', 'Adam', 'AdamW'], default='SGD', help='optimizer')
    # å¯ç”¨åŒæ­¥æ‰¹è§„èŒƒåŒ–ï¼Œåªåœ¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰æ¨¡å¼ä¸‹å¯ç”¨ï¼Œæ˜¯å¦å¼€å¯è·¨å¡åŒæ­¥BNï¼›å¼€å¯å‚æ•°åå³å¯ä½¿ç”¨SyncBatchNormå¤š GPU è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    # æŒ‡å®šæ•°æ®åŠ è½½å™¨çš„æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œä»…åœ¨åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDPï¼‰æ¨¡å¼ä¸‹æœ‰ç”¨
    # è¿™é‡Œç»å¸¸å‡ºé—®é¢˜ï¼ŒWindowsç³»ç»ŸæŠ¥é”™æ—¶å¯ä»¥è®¾ç½®æˆ0
    parser.add_argument('--workers', type=int, default=2, help='max dataloader workers (per RANK in DDP mode)')
    # æŒ‡å®šä¿å­˜è®­ç»ƒç»“æœçš„è·¯å¾„
    parser.add_argument('--project', default=ROOT / 'runs/train', help='save to project/name')
    # é‡å‘½åæ–‡ä»¶å
    parser.add_argument('--name', default='exp', help='save to project/name')
    #å…è®¸ä½¿ç”¨ç°æœ‰çš„project/nameï¼Œä¸å¢åŠ è®¡æ•°å™¨
    parser.add_argument('--exist-ok', action='store_true', help='existing project/name ok, do not increment')
    # ä½¿ç”¨å››è·¯æ•°æ®åŠ è½½å™¨ï¼Œå®˜æ–¹å‘å¸ƒçš„å¼€å¯è¿™ä¸ªåŠŸèƒ½åçš„å®é™…æ•ˆæœï¼Œå¥½å¤„æ˜¯åœ¨æ¯”é»˜è®¤640å¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ•ˆæœæ›´å¥½ï¼Œå‰¯ä½œç”¨æ˜¯åœ¨640å¤§å°çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ•ˆæœå¯èƒ½ä¼šå·®ä¸€äº›
    parser.add_argument('--quad', action='store_true', help='quad dataloader')
    # ä½¿ç”¨ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨
    parser.add_argument('--cos-lr', action='store_true', help='cosine LR scheduler')
    # æ ‡ç­¾å¹³æ»‘çš„epsilonå€¼ï¼Œæ˜¯å¦å¯¹æ ‡ç­¾è¿›è¡Œå¹³æ»‘å¤„ç†ï¼Œé»˜è®¤æ˜¯ä¸å¯ç”¨çš„
    # åœ¨è®­ç»ƒæ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸èƒ½ä¿è¯æ‰€æœ‰sampleéƒ½æ ‡æ³¨æ­£ç¡®ï¼Œå¦‚æœæŸä¸ªæ ·æœ¬æ ‡æ³¨é”™è¯¯ï¼Œå°±å¯èƒ½äº§ç”Ÿè´Ÿé¢å°è±¡ï¼Œå¦‚æœæˆ‘ä»¬æœ‰åŠæ³•â€œå‘Šè¯‰â€æ¨¡å‹ï¼Œæ ·æœ¬çš„æ ‡ç­¾ä¸ä¸€å®šæ­£ç¡®ï¼Œ
    # é‚£ä¹ˆè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹å¯¹äºå°‘é‡çš„æ ·æœ¬é”™è¯¯å°±ä¼šæœ‰â€œå…ç–«åŠ›â€é‡‡ç”¨éšæœºåŒ–çš„æ ‡ç­¾ä½œä¸ºè®­ç»ƒæ•°æ®æ—¶ï¼ŒæŸå¤±å‡½æ•°æœ‰1-Îµçš„æ¦‚ç‡ä¸ä¸Šé¢çš„å¼å­ç›¸åŒï¼Œ
    # æ¯”å¦‚è¯´å‘Šè¯‰æ¨¡å‹åªæœ‰0.95æ¦‚ç‡æ˜¯é‚£ä¸ªæ ‡ç­¾
    parser.add_argument('--label-smoothing', type=float, default=0.0, help='Label smoothing epsilon')
    # EarlyStoppingçš„è€å¿ƒåº¦ï¼ˆæ²¡æœ‰æ”¹è¿›çš„æ—¶æœŸæ•°ï¼‰ï¼Œå¦‚æœæ¨¡å‹åœ¨defaultå€¼è½®æ•°é‡Œæ²¡æœ‰æå‡ï¼Œåˆ™åœæ­¢è®­ç»ƒæ¨¡å‹
    parser.add_argument('--patience', type=int, default=100, help='EarlyStopping patience (epochs without improvement)')
    # å†»ç»“å±‚æ¬¡æ•°é‡ï¼Œå¦‚æŒ‡å®šbackbone = 10ï¼Œfirst3 = 0ã€1ã€2ï¼Œå¯ä»¥åœ¨yolov5s.yamlä¸­æŸ¥çœ‹ä¸»å¹²ç½‘ç»œå±‚æ•°
    # å†»ç»“è®­ç»ƒæ˜¯è¿ç§»å­¦ä¹ å¸¸ç”¨çš„æ–¹æ³•ï¼Œå½“æˆ‘ä»¬åœ¨ä½¿ç”¨æ•°æ®é‡ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œé€šå¸¸æˆ‘ä»¬ä¼šé€‰æ‹©å…¬å…±æ•°æ®é›†æä¾›æƒé‡ä½œä¸ºé¢„è®­ç»ƒæƒé‡ï¼Œ
    # æˆ‘ä»¬çŸ¥é“ç½‘ç»œçš„backboneä¸»è¦æ˜¯ç”¨æ¥æå–ç‰¹å¾ç”¨çš„ï¼Œä¸€èˆ¬å¤§å‹æ•°æ®é›†è®­ç»ƒå¥½çš„æƒé‡ä¸»å¹²ç‰¹å¾æå–èƒ½åŠ›æ˜¯æ¯”è¾ƒå¼ºçš„ï¼Œ
    # è¿™ä¸ªæ—¶å€™æˆ‘ä»¬åªéœ€è¦å†»ç»“ä¸»å¹²ç½‘ç»œï¼Œfine-tuneåé¢å±‚å°±å¯ä»¥äº†ï¼Œä¸éœ€è¦ä»å¤´å¼€å§‹è®­ç»ƒï¼Œå¤§å¤§å‡å°‘äº†å®è·µè€Œä¸”è¿˜æé«˜äº†æ€§èƒ½
    parser.add_argument('--freeze', nargs='+', type=int, default=[0], help='Freeze layers: backbone=10, first3=0 1 2')
    # æ¯xä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå°äº1ï¼Œåˆ™ç¦ç”¨ï¼‰ï¼Œç”¨äºè®¾ç½®å¤šå°‘ä¸ªepochä¿å­˜ä¸€ä¸‹checkpointï¼›
    parser.add_argument('--save-period', type=int, default=-1, help='Save checkpoint every x epochs (disabled if < 1)')
    # å…¨å±€è®­ç»ƒç§å­
    parser.add_argument('--seed', type=int, default=0, help='Global training seed')
    # è‡ªåŠ¨ DDP å¤š GPU å‚æ•°ï¼Œä¸è¦ä¿®æ”¹ï¼Œå•GPUè®¾å¤‡ä¸éœ€è¦è®¾ç½®
    parser.add_argument('--local_rank', type=int, default=-1, help='Automatic DDP Multi-GPU argument, do not modify')

    # Logger arguments
    # å®ä½“åç§°ï¼Œåœ¨çº¿å¯è§†åŒ–å·¥å…·ï¼Œç±»ä¼¼äºtensorboard
    parser.add_argument('--entity', default=None, help='Entity')
    # ä¸Šä¼ æ•°æ®é›†ã€‚å¦‚æœæŒ‡å®šäº† 'val' é€‰é¡¹ï¼Œåˆ™ä»…ä¸Šä¼ éªŒè¯æ•°æ®é›†
    # æ˜¯å¦ä¸Šä¼ datasetåˆ°wandb tabel(å°†æ•°æ®é›†ä½œä¸ºäº¤äº’å¼ dsvizè¡¨ åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹ã€æŸ¥è¯¢ã€ç­›é€‰å’Œåˆ†ææ•°æ®é›†) é»˜è®¤False
    parser.add_argument('--upload_dataset', nargs='?', const=True, default=False, help='Upload data, "val" option')
    # è®¾ç½®è®°å½•è¾¹ç•Œæ¡†å›¾åƒçš„é—´éš”
    parser.add_argument('--bbox_interval', type=int, default=-1, help='Set bounding-box image logging interval')
    # è¦ä½¿ç”¨çš„æ•°æ®é›†å·¥ä»¶çš„ç‰ˆæœ¬åç§°ã€‚é»˜è®¤ä¸º 'latest'ï¼Œè¿™ä¸ªåŠŸèƒ½ä½œè€…è¿˜æœªå®ç°
    parser.add_argument('--artifact_alias', type=str, default='latest', help='Version of dataset artifact to use')
    # åœ¨è§£æè¿‡ç¨‹ä¸­é‡åˆ°ä¸è®¤è¯†çš„å‚æ•°ï¼Œparse_args() å‡½æ•°ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œè€Œ parse_known_args() å‡½æ•°åˆ™ä¼šå°†ä¸è®¤è¯†çš„å‚æ•°å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­è¿”å›ï¼ŒåŒæ—¶è¿”å›å·²çŸ¥å‚æ•°çš„å€¼
    return parser.parse_known_args()[0] if known else parser.parse_args()


def main(opt, callbacks=Callbacks()):
    # Checks
    if RANK in {-1, 0}:
        print_args(vars(opt))
        check_git_status()
        check_requirements()

    # ç”¨äºæ¢å¤æ¨¡å‹è®­ç»ƒ
    if opt.resume and not check_comet_resume(opt) and not opt.evolve:
        # å¦‚æœè¿™ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ç±»å‹ï¼Œé‚£ä¹ˆå°±è°ƒç”¨check_file()å‡½æ•°æ£€æŸ¥è¿™ä¸ªæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å°±ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
        # å¦‚æœè¿™ä¸ªå‚æ•°æ˜¯Noneæˆ–è€…å…¶ä»–éå­—ç¬¦ä¸²ç±»å‹ï¼Œé‚£ä¹ˆå°±è°ƒç”¨get_latest_run()å‡½æ•°è·å–æœ€æ–°çš„checkpointæ–‡ä»¶ã€‚
        last = Path(check_file(opt.resume) if isinstance(opt.resume, str) else get_latest_run())
        opt_yaml = last.parent.parent / 'opt.yaml'  # train options yaml
        opt_data = opt.data  # original dataset
        if opt_yaml.is_file():
            with open(opt_yaml, errors='ignore') as f:
                # å°† opt.yaml æ–‡ä»¶è§£æä¸ºä¸€ä¸ª Python å­—å…¸
                d = yaml.safe_load(f)
        else:
            # ä»æŒ‡å®šçš„last.ptæ–‡ä»¶ä¸­åŠ è½½è®­ç»ƒæœŸé—´çš„å„ç§å‚æ•°å’Œé€‰é¡¹ï¼Œmap_locationæŒ‡å®šäº†åŠ è½½å¯¹è±¡çš„ä½ç½®ï¼Œè¿™é‡ŒæŒ‡å®šä¸ºCPU
            d = torch.load(last, map_location='cpu')['opt']
        # å°†å­—å…¸è½¬æ¢ä¸ºå‘½åç©ºé—´ï¼Œå°†æ–°çš„é…ç½®å‚æ•°å‘½åç©ºé—´èµ‹å€¼ç»™ opt å˜é‡ï¼Œåé¢å¯ä»¥ç»§ç»­ä½¿ç”¨è¿™äº›å‚æ•°è¿›è¡Œè®­ç»ƒäº†
        opt = argparse.Namespace(**d)  # replace
        opt.cfg, opt.weights, opt.resume = '', str(last), True  # reinstate
        if is_url(opt_data):
            opt.data = check_file(opt_data)  # è·å–è¯¥URLæ‰€æŒ‡å‘çš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œä»¥é¿å…HUBæ¢å¤é‰´æƒè¶…æ—¶
    else:
        # æ£€æŸ¥æŒ‡å®šçš„æ•°æ®é›†ã€é…ç½®æ–‡ä»¶ã€è¶…å‚æ•°æ–‡ä»¶å’Œæƒé‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶è¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–
        opt.data, opt.cfg, opt.hyp, opt.weights, opt.project = \
            check_file(opt.data), check_yaml(opt.cfg), check_yaml(opt.hyp), str(opt.weights), str(opt.project)  # checks
        assert len(opt.cfg) or len(opt.weights), 'either --cfg or --weights must be specified'
        if opt.evolve:
            if opt.project == str(ROOT / 'runs/train'):  # if default project name, rename to runs/evolve
                opt.project = str(ROOT / 'runs/evolve')
            opt.exist_ok, opt.resume = opt.resume, False  # pass resume to exist_ok and disable resume
        if opt.name == 'cfg':
            opt.name = Path(opt.cfg).stem  # Pathå¯¹è±¡è¡¨ç¤ºæ–‡ä»¶è·¯å¾„ã€‚.stemæ˜¯è¿™ä¸ªå¯¹è±¡çš„ä¸€ä¸ªå±æ€§ï¼Œå®ƒè¿”å›è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸”ä¸åŒ…æ‹¬æ–‡ä»¶æ‰©å±•å
        # åˆå§‹åŒ–ä¸ºæŒ‡å®šçš„é¡¹ç›®åç§°å’Œæ¨¡å‹åç§°ç»„æˆçš„è·¯å¾„ï¼Œå¹¶ä½¿ç”¨ increment_path å‡½æ•°åœ¨è¯¥è·¯å¾„ä¸Šè¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœè¯¥è·¯å¾„å·²ç»å­˜åœ¨ï¼Œåˆ™ä¼šè‡ªåŠ¨åŠ ä¸Šåç¼€ _1ã€_2 ç­‰ï¼‰
        opt.save_dir = str(increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok))

    # DDP mode
    device = select_device(opt.device, batch_size=opt.batch_size)
    if LOCAL_RANK != -1:
        msg = 'is not compatible with YOLOv5 Multi-GPU DDP training'
        # ä¸¤ä¸ªé€‰é¡¹ä¸æ”¯æŒå¤šGPUæ¨¡å¼
        assert not opt.image_weights, f'--image-weights {msg}'
        assert not opt.evolve, f'--evolve {msg}'
        # åˆ¤æ–­æ˜¯å¦è®¾ç½®äº†--batch-size -1é€‰é¡¹ï¼Œè‹¥è®¾ç½®äº†åˆ™éœ€è¦ä¼ å…¥ä¸€ä¸ªæœ‰æ•ˆçš„--batch-sizeé€‰é¡¹
        assert opt.batch_size != -1, f'AutoBatch with --batch-size -1 {msg}, please pass a valid --batch-size'
        # åˆ¤æ–­--batch-sizeæ˜¯å¦æ˜¯WORLD_SIZEçš„å€æ•°
        assert opt.batch_size % WORLD_SIZE == 0, f'--batch-size {opt.batch_size} must be multiple of WORLD_SIZE'
        # å¦‚æœå½“å‰æœºå™¨ä¸Šçš„GPUæ•°é‡å¤§äºLOCAL_RANKï¼Œåˆ™ä¼šå°†å½“å‰è®¾å¤‡çš„ç¼–å·è®¾ç½®ä¸ºLOCAL_RANK
        assert torch.cuda.device_count() > LOCAL_RANK, 'insufficient CUDA devices for DDP command'
        # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„ GPU ID
        torch.cuda.set_device(LOCAL_RANK)
        # å°†å½“å‰è¿›ç¨‹çš„è®¾å¤‡ç±»å‹è®¾ç½®ä¸º CUDAï¼ŒåŒæ—¶æŒ‡å®šäº† GPU ID
        device = torch.device('cuda', LOCAL_RANK)
        # åˆå§‹åŒ–äº†è¿›ç¨‹ç»„ï¼Œæ ¹æ®ç³»ç»Ÿçš„ä¸åŒï¼Œæ”¯æŒ nccl æˆ–è€… gloo çš„æ–¹å¼æ¥å®ç°è¿›ç¨‹é€šä¿¡ï¼Œä»¥å®ç°å¤šGPUè®­ç»ƒ
        dist.init_process_group(backend="nccl" if dist.is_nccl_available() else "gloo")
        # nccl ï¼ˆNVIDIA Collective Communications Libraryï¼‰æ˜¯ä¸€ç§é¢å‘å¤š GPU çš„é€šä¿¡åº“ï¼Œ
        # å®ƒæ˜¯ NVIDIA æä¾›çš„é«˜æ•ˆçš„é›†ä½“é€šä¿¡åº“ï¼Œå¯ä»¥ç”¨äºå¿«é€Ÿçš„æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œã€‚nccl é€šè¿‡ NVIDIA GPU ä¸Šçš„ç¡¬ä»¶åŠ é€Ÿç½‘ç»œå®ç°äº†é«˜æ•ˆçš„å¤š GPU é€šä¿¡ã€‚
        # gloo åˆ™æ˜¯ä¸€ç§å¤šèŠ‚ç‚¹çš„ CPU/GPU é€šä¿¡åç«¯ï¼Œå®ƒä½¿ç”¨äº†ç±»ä¼¼äº MapReduce çš„æ€è·¯æ¥å®ç°æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œã€‚
        # gloo çš„å®ç°ä¸ä¾èµ–äºç¡¬ä»¶ç‰¹æ€§ï¼Œå› æ­¤å¯ä»¥åœ¨å¤§å¤šæ•°å¹³å°ä¸Šä½¿ç”¨ã€‚

    # è®­ç»ƒ
    if not opt.evolve:
        train(opt.hyp, opt, device, callbacks)

    # æ¼”åŒ–è¶…å‚æ•°ï¼ˆå¯é€‰ï¼‰
    else:
        # è¶…å‚æ•°è¿›åŒ–å…ƒæ•°æ®ï¼ˆå˜å¼‚å°ºåº¦0-1ã€ä¸‹é™ã€ä¸Šé™ï¼‰
        meta = {
            'lr0': (1, 1e-5, 1e-1),  # initial learning rate (SGD=1E-2, Adam=1E-3)
            'lrf': (1, 0.01, 1.0),  # final OneCycleLR learning rate (lr0 * lrf)
            'momentum': (0.3, 0.6, 0.98),  # SGD momentum/Adam beta1
            'weight_decay': (1, 0.0, 0.001),  # optimizer weight decay
            'warmup_epochs': (1, 0.0, 5.0),  # warmup epochs (fractions ok)
            'warmup_momentum': (1, 0.0, 0.95),  # warmup initial momentum
            'warmup_bias_lr': (1, 0.0, 0.2),  # warmup initial bias lr
            'box': (1, 0.02, 0.2),  # box loss gain
            'cls': (1, 0.2, 4.0),  # cls loss gain
            'cls_pw': (1, 0.5, 2.0),  # cls BCELoss positive_weight
            'obj': (1, 0.2, 4.0),  # obj loss gain (scale with pixels)
            'obj_pw': (1, 0.5, 2.0),  # obj BCELoss positive_weight
            'iou_t': (0, 0.1, 0.7),  # IoU training threshold
            'anchor_t': (1, 2.0, 8.0),  # anchor-multiple threshold
            'anchors': (2, 2.0, 10.0),  # anchors per output grid (0 to ignore)
            'fl_gamma': (0, 0.0, 2.0),  # focal loss gamma (efficientDet default gamma=1.5)
            'hsv_h': (1, 0.0, 0.1),  # image HSV-Hue augmentation (fraction)
            'hsv_s': (1, 0.0, 0.9),  # image HSV-Saturation augmentation (fraction)
            'hsv_v': (1, 0.0, 0.9),  # image HSV-Value augmentation (fraction)
            'degrees': (1, 0.0, 45.0),  # image rotation (+/- deg)
            'translate': (1, 0.0, 0.9),  # image translation (+/- fraction)
            'scale': (1, 0.0, 0.9),  # image scale (+/- gain)
            'shear': (1, 0.0, 10.0),  # image shear (+/- deg)
            'perspective': (0, 0.0, 0.001),  # image perspective (+/- fraction), range 0-0.001
            'flipud': (1, 0.0, 1.0),  # image flip up-down (probability)
            'fliplr': (0, 0.0, 1.0),  # image flip left-right (probability)
            'mosaic': (1, 0.0, 1.0),  # image mixup (probability)
            'mixup': (1, 0.0, 1.0),  # image mixup (probability)
            'copy_paste': (1, 0.0, 1.0)}  # segment copy-paste (probability)

        # æ‰“å¼€ç”¨æˆ·æä¾›çš„è¶…å‚æ•°æ–‡ä»¶(opt.hyp)
        with open(opt.hyp, errors='ignore') as f:
            hyp = yaml.safe_load(f)  # åŠ è½½ä¸ºä¸€ä¸ªPythonå­—å…¸å¯¹è±¡ï¼Œä»¥ä¾¿ç¨ååœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨
            # å¦‚æœè¯¥æ–‡ä»¶ä¸­æ²¡æœ‰æŒ‡å®šé”šæ¡†(anchor boxes)çš„æ•°é‡ï¼Œé‚£ä¹ˆé»˜è®¤è®¾ç½®ä¸º3ä¸ª
            if 'anchors' not in hyp:
                hyp['anchors'] = 3
        # è¡¨ç¤ºä¸è‡ªåŠ¨ç”Ÿæˆanchors
        if opt.noautoanchor:
            # éœ€è¦ä»è¶…å‚æ•°å­—å…¸hypå’Œå…ƒæ•°æ®å­—å…¸metaä¸­å°†anchorsé¡¹åˆ é™¤
            del hyp['anchors'], meta['anchors']
        # å°†å®ƒä»¬è®¾ç½®ä¸ºTrueæ—¶ï¼Œè¡¨ç¤ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åªéªŒè¯å¹¶ä¿å­˜æœ€ç»ˆepochçš„æ¨¡å‹
        opt.noval, opt.nosave, save_dir = True, True, Path(opt.save_dir)  # only val/save final epoch
        # eiæŒ‡ç¤ºhypå­—å…¸ä¸­å“ªäº›å€¼æ˜¯å¯è¿›åŒ–çš„ï¼Œé€šè¿‡è¿­ä»£hypå­—å…¸ä¸­çš„å€¼å¹¶ä½¿ç”¨isinstanceï¼ˆï¼‰å‡½æ•°æ£€æŸ¥æ¯ä¸ªå€¼æ˜¯å¦æ˜¯intæˆ–floatçš„å®ä¾‹æ¥åˆ›å»ºeiã€‚
        # ei = [isinstance(x, (int, float)) for x in hyp.values()]  # evolvable indices
        evolve_yaml, evolve_csv = save_dir / 'hyp_evolve.yaml', save_dir / 'evolve.csv'
        if opt.bucket:
            # ä¸‹è½½ä¸€ä¸ªåä¸º"evolve.csv"çš„æ–‡ä»¶åˆ°æœ¬åœ°çš„æŒ‡å®šç›®å½•ä¸­
            # é€šè¿‡è¿è¡Œç³»ç»Ÿå‘½ä»¤æ¥æ‰§è¡Œæ­¤æ“ä½œï¼Œgsutilå‘½ä»¤ç”¨äºè®¿é—®å’Œç®¡ç† GCS ä¸Šçš„å¯¹è±¡ï¼Œcpå‚æ•°ç”¨äºå¤åˆ¶æ–‡ä»¶ã€‚å¦‚æœæœ¬åœ°ç›®å½•ä¸‹å·²ç»æœ‰ç›¸åŒçš„æ–‡ä»¶ï¼Œå°†ä¼šè¢«è¦†ç›–
            subprocess.run([
                'gsutil',
                'cp',
                f'gs://{opt.bucket}/evolve.csv',
                str(evolve_csv)])

        # è¿›è¡ŒYOLOv5çš„è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–ï¼Œå³è¿›è¡Œè¿›åŒ–ç®—æ³•ï¼Œç”Ÿæˆå¤šä¸ªæ–°çš„è¶…å‚æ•°ç»„åˆå¹¶è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œæ ¹æ®è¿™äº›æ¨¡å‹çš„è¡¨ç°é€‰æ‹©å‡ºè¡¨ç°æœ€å¥½çš„è¶…å‚æ•°ç»„åˆï¼Œå¹¶ç»§ç»­è¿›åŒ–ã€‚
        # å…¶ä¸­ï¼Œå¾ªç¯æ¬¡æ•°ä¸º opt.evolve æ¬¡ï¼Œæ¯æ¬¡è¿›åŒ–éƒ½ä¼šç”Ÿæˆæ–°çš„è¶…å‚æ•°ç»„åˆ
        for _ in range(opt.evolve):
            if evolve_csv.exists():  # å¦‚æœevolve.csvå­˜åœ¨ï¼šé€‰æ‹©æœ€ä½³è¶…å‚æ•°å¹¶è¿›è¡Œå˜å¼‚
                # é€‰æ‹©çˆ¶ä»£
                parent = 'single'  # çˆ¶ä»£é€‰æ‹©æ–¹æ³•ï¼š'single' æˆ–è€… 'weighted'
                # ä» evolve.csv æ–‡ä»¶ä¸­è¯»å–å·²æœ‰çš„è¶…å‚æ•°å’Œå®ƒä»¬çš„è¯„ä¼°ç»“æœ
                # è·³è¿‡ç¬¬ä¸€è¡Œï¼ˆå› ä¸ºç¬¬ä¸€è¡Œæ˜¯åˆ—åï¼‰ï¼Œå°†å‰©ä½™çš„è¡Œä½œä¸ºäºŒç»´æ•°ç»„è¿”å›ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªè¶…å‚æ•°çš„å€¼ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè¶…å‚æ•°ç»„åˆçš„è¯„ä¼°ç»“æœ
                # åœ¨è¿™æ®µä»£ç ä¸­ï¼Œndmin=2 æ„å‘³ç€è¿”å›çš„æ•°ç»„è‡³å°‘æ˜¯äºŒç»´çš„ï¼Œå³ä½¿åªæœ‰ä¸€è¡Œæˆ–ä¸€åˆ—ï¼Œè¿™ä½¿å¾—ä»£ç æ›´åŠ å¥å£®
                x = np.loadtxt(evolve_csv, ndmin=2, delimiter=',', skiprows=1)
                n = min(5, len(x))  # è€ƒè™‘çš„ä»¥å‰ç»“æœæ•°é‡ï¼Œä»ä¹‹å‰çš„æµ‹è¯•ç»“æœä¸­æœ€å¤šé€‰æ‹©5ä¸ªè¿›è¡Œè¿›åŒ–
                x = x[np.argsort(-fitness(x))][:n]  # æœ€é«˜nä¸ªå˜å¼‚ï¼ŒæŒ‰ç…§æµ‹è¯•æ€§èƒ½ä»é«˜åˆ°ä½æ’åºï¼Œå¹¶è¿”å›ä¸‹æ ‡
                # è®¡ç®—æ¯ä¸ªæµ‹è¯•ç»“æœçš„é€‚åº”åº¦ï¼Œç„¶åå¯¹é€‚åº”åº¦è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œç¡®ä¿æ‰€æœ‰çš„æƒé‡å€¼éƒ½å¤§äº0ï¼ŒåŠ ä¸Šäº†ä¸€ä¸ªå¾ˆå°çš„æ•°ï¼ˆ1E-6ï¼‰æ¥é¿å…æƒé‡å‡ºç°0çš„æƒ…å†µ
                w = fitness(x) - fitness(x).min() + 1E-6  # æƒé‡ï¼ˆæ€»å’Œ > 0ï¼‰
                # å¦‚æœparentç­‰äº'single'æˆ–è€…å‰å‡ ä»£ä¸­åªæœ‰ä¸€ä¸ªç»“æœï¼Œå°±ä½¿ç”¨éšæœºé€‰æ‹©çš„æ–¹æ³•ä»å‰å‡ ä»£çš„ç»“æœä¸­é€‰æ‹©ä¸€ä¸ªä½œä¸ºçˆ¶ä»£
                if parent == 'single' or len(x) == 1:
                    # x = x[random.randint(0, n - 1)]  # random selection
                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection
                elif parent == 'weighted':
                    # ä½¿ç”¨åŠ æƒå¹³å‡çš„æ–¹æ³•é€‰æ‹©çˆ¶ä»£ï¼Œå…¶ä¸­æ¯ä¸ªå‰å‡ ä»£çš„ç»“æœéƒ½æŒ‰ç…§å…¶é€‚åº”åº¦å€¼ä½œä¸ºæƒé‡ï¼ŒåŠ æƒå¹³å‡å¾—åˆ°ä¸€ä¸ªæ–°çš„å‚æ•°ç»„åˆ
                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination

                # å¯¹è¶…å‚æ•°çš„å˜å¼‚æ“ä½œ
                mp, s = 0.8, 0.2  # mpæ˜¯å˜å¼‚æ¦‚ç‡ï¼Œsæ˜¯å˜å¼‚çš„å¹…åº¦
                npr = np.random
                npr.seed(int(time.time()))
                g = np.array([meta[k][0] for k in hyp.keys()])  # å½“å‰è¶…å‚æ•°å€¼åœ¨0åˆ°1ä¹‹é—´çš„å¢ç›Šå› å­
                ng = len(meta)
                v = np.ones(ng)  # væ˜¯å½“å‰çš„å˜å¼‚å‘é‡ï¼Œåˆå§‹å€¼éƒ½æ˜¯1ã€‚åœ¨å˜å¼‚è¿‡ç¨‹ä¸­ï¼Œç¨‹åºä¼šåœ¨æ¯ä¸ªè¶…å‚æ•°ä¸Šåˆ†åˆ«è¿›è¡Œå˜å¼‚
                while all(v == 1):  # å˜å¼‚ç›´åˆ°å‘ç”Ÿå˜åŒ–ï¼ˆé˜²æ­¢é‡å¤ï¼‰
                    # åˆ¤æ–­æ˜¯å¦è¿›è¡Œå˜å¼‚çš„æ¦‚ç‡æ˜¯å¦å¤§äºéšæœºç”Ÿæˆçš„ä¸€ä¸ª0åˆ°1ä¹‹é—´çš„éšæœºæ•°ï¼Œ
                    # ç„¶åæ ¹æ®å˜å¼‚çš„æ¦‚ç‡å’Œå¹…åº¦ç”Ÿæˆä¸€ä¸ªéšæœºæ•°ä½œä¸ºå½“å‰è¶…å‚æ•°çš„å˜å¼‚å› å­ï¼Œæœ€åå°†å½“å‰è¶…å‚æ•°ä¹˜ä»¥å˜å¼‚å› å­è¿›è¡Œå˜å¼‚
                    # å¦‚æœå˜å¼‚åçš„è¶…å‚æ•°ä¸åœ¨0.3åˆ°3ä¹‹é—´ï¼Œç¨‹åºä¼šå°†å…¶å‰ªåˆ‡åˆ°è¿™ä¸ªåŒºé—´å†…
                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)
                # å°†ç»è¿‡é€‰å®šçš„å˜å¼‚ç­–ç•¥åçš„æ–°å‚æ•°åº”ç”¨åˆ°æ¨¡å‹çš„è¶…å‚æ•°ä¸Š
                # åŸºå› ä¼šæ ¹æ®æ¦‚ç‡å’Œæ ‡å‡†å·®è¿›è¡Œå˜å¼‚ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„åŸºå› å€¼ï¼Œç„¶åå°†è¿™ä¸ªæ–°çš„åŸºå› å€¼ä¹˜ä»¥ä¸€ä¸ªå˜å¼‚å› å­ v[i]ï¼Œå°†å¾—åˆ°çš„ç»“æœæ›´æ–°åˆ°è¶…å‚æ•° hyp çš„å¯¹åº”é”® k ä¸Š
                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)
                    hyp[k] = float(x[i + 7] * v[i])  # mutate

            # å°†æ¯ä¸ªå‚æ•°çš„å–å€¼é™åˆ¶åœ¨å…¶å…è®¸çš„èŒƒå›´å†…ï¼Œå¹¶å°†å…¶å››èˆäº”å…¥åˆ°å°æ•°ç‚¹åäº”ä½
            for k, v in meta.items():
                hyp[k] = max(hyp[k], v[1])  # ä½äºå…¶è§„å®šçš„ä¸‹é™ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºå…¶ä¸‹é™
                hyp[k] = min(hyp[k], v[2])  # é«˜äºå…¶è§„å®šçš„ä¸Šé™ï¼Œåˆ™å°†å…¶è®¾ç½®ä¸ºå…¶ä¸Šé™
                hyp[k] = round(hyp[k], 5)  # å°†æ¯ä¸ªå‚æ•°çš„å€¼èˆå…¥åˆ°å°æ•°ç‚¹åäº”ä½

            # è®­ç»ƒ
            # é¦–å…ˆï¼Œå°†å½“å‰çš„è¶…å‚æ•°ï¼ˆhypï¼‰å¤åˆ¶ä¸€ä»½ï¼Œä»¥å…å¯¹åŸå§‹è¶…å‚æ•°è¿›è¡Œæ›´æ”¹ã€‚ç„¶åä½¿ç”¨å½“å‰è¶…å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œå°†è®­ç»ƒç»“æœå­˜å‚¨åœ¨resultså˜é‡ä¸­ã€‚
            results = train(hyp.copy(), opt, device, callbacks)
            # åˆ›å»ºä¸€ä¸ªç©ºçš„Callbackså¯¹è±¡ï¼Œä»¥ç¡®ä¿ä¸ä¼šä½¿ç”¨ä¸Šä¸€æ¬¡çš„å›è°ƒ
            callbacks = Callbacks()
            # è°ƒç”¨print_mutation()å‡½æ•°ï¼Œå°†å…³é”®æŒ‡æ ‡ï¼ˆprecisionã€recallã€mAPç­‰ï¼‰çš„å€¼ã€è¶…å‚æ•°å’Œä¿å­˜ç›®å½•ä½œä¸ºå‚æ•°ä¼ é€’ï¼Œä»¥å°†ç»“æœå†™å…¥æ—¥å¿—æ–‡ä»¶ä¸­ï¼Œç”¨äºåç»­åˆ†æå’Œå‚è€ƒ
            keys = ('metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95', 'val/box_loss',
                    'val/obj_loss', 'val/cls_loss')
            print_mutation(keys, results, hyp.copy(), save_dir, opt.bucket)

        # ç”¨äºç»˜åˆ¶è¶…å‚æ•°è¿›åŒ–è¿‡ç¨‹ä¸­çš„ç»“æœï¼Œå¹¶è¾“å‡ºè®­ç»ƒçš„ç»“æœä¿å­˜è·¯å¾„å’Œä½¿ç”¨çš„å‘½ä»¤è¡Œå‚æ•°ç¤ºä¾‹
        plot_evolve(evolve_csv)
        LOGGER.info(f'Hyperparameter evolution finished {opt.evolve} generations\n'
                    f"Results saved to {colorstr('bold', save_dir)}\n"
                    f'Usage example: $ python train.py --hyp {evolve_yaml}')


def run(**kwargs):
    # Usage: import train; train.run(data='coco128.yaml', imgsz=320, weights='yolov5m.pt')
    opt = parse_opt(True)
    for k, v in kwargs.items():
        setattr(opt, k, v)
    main(opt)
    return opt


if __name__ == "__main__":
    opt = parse_opt()
    main(opt)
